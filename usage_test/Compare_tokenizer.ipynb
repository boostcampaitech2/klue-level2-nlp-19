{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd42b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>{'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...</td>\n",
       "      <td>{'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>{'word': '민주평화당', 'start_idx': 19, 'end_idx': ...</td>\n",
       "      <td>{'word': '대안신당', 'start_idx': 14, 'end_idx': 1...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>{'word': '광주FC', 'start_idx': 21, 'end_idx': 2...</td>\n",
       "      <td>{'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>{'word': '아성다이소', 'start_idx': 13, 'end_idx': ...</td>\n",
       "      <td>{'word': '박정부', 'start_idx': 22, 'end_idx': 24...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>{'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...</td>\n",
       "      <td>{'word': '1967', 'start_idx': 0, 'end_idx': 3,...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32465</th>\n",
       "      <td>32465</td>\n",
       "      <td>한국당은 7일 오전 9시부터 오후 5시까지 진행된 원내대표 및 정책위의장 후보자 등...</td>\n",
       "      <td>{'word': '유기준', 'start_idx': 93, 'end_idx': 95...</td>\n",
       "      <td>{'word': '부산 서구·동구', 'start_idx': 100, 'end_id...</td>\n",
       "      <td>per:employee_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32466</th>\n",
       "      <td>32466</td>\n",
       "      <td>법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...</td>\n",
       "      <td>{'word': '최시형', 'start_idx': 7, 'end_idx': 9, ...</td>\n",
       "      <td>{'word': '손병희', 'start_idx': 17, 'end_idx': 19...</td>\n",
       "      <td>per:colleagues</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32467</th>\n",
       "      <td>32467</td>\n",
       "      <td>완도군(군수 신우철)이 국토교통부에서 실시한 '2019 교통문화지수 실태조사'에서 ...</td>\n",
       "      <td>{'word': '완도군', 'start_idx': 0, 'end_idx': 2, ...</td>\n",
       "      <td>{'word': '신우철', 'start_idx': 7, 'end_idx': 9, ...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32468</th>\n",
       "      <td>32468</td>\n",
       "      <td>중앙일보, JTBC 회장을 지낸 이후 중앙홀딩스 회장, 재단법인 한반도평화만들기 이...</td>\n",
       "      <td>{'word': 'JTBC', 'start_idx': 6, 'end_idx': 9,...</td>\n",
       "      <td>{'word': '중앙홀딩스', 'start_idx': 21, 'end_idx': ...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32469</th>\n",
       "      <td>32469</td>\n",
       "      <td>화순군(군수 구충곤)은 17일 동면의 이장 20여 명이 코로나 19 예방을 위해 버...</td>\n",
       "      <td>{'word': '화순군', 'start_idx': 0, 'end_idx': 2, ...</td>\n",
       "      <td>{'word': '구충곤', 'start_idx': 7, 'end_idx': 9, ...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32470 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "0          0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...   \n",
       "1          1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...   \n",
       "2          2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...   \n",
       "3          3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...   \n",
       "4          4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...   \n",
       "...      ...                                                ...   \n",
       "32465  32465  한국당은 7일 오전 9시부터 오후 5시까지 진행된 원내대표 및 정책위의장 후보자 등...   \n",
       "32466  32466  법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...   \n",
       "32467  32467  완도군(군수 신우철)이 국토교통부에서 실시한 '2019 교통문화지수 실태조사'에서 ...   \n",
       "32468  32468  중앙일보, JTBC 회장을 지낸 이후 중앙홀딩스 회장, 재단법인 한반도평화만들기 이...   \n",
       "32469  32469  화순군(군수 구충곤)은 17일 동면의 이장 20여 명이 코로나 19 예방을 위해 버...   \n",
       "\n",
       "                                          subject_entity  \\\n",
       "0      {'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...   \n",
       "1      {'word': '민주평화당', 'start_idx': 19, 'end_idx': ...   \n",
       "2      {'word': '광주FC', 'start_idx': 21, 'end_idx': 2...   \n",
       "3      {'word': '아성다이소', 'start_idx': 13, 'end_idx': ...   \n",
       "4      {'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...   \n",
       "...                                                  ...   \n",
       "32465  {'word': '유기준', 'start_idx': 93, 'end_idx': 95...   \n",
       "32466  {'word': '최시형', 'start_idx': 7, 'end_idx': 9, ...   \n",
       "32467  {'word': '완도군', 'start_idx': 0, 'end_idx': 2, ...   \n",
       "32468  {'word': 'JTBC', 'start_idx': 6, 'end_idx': 9,...   \n",
       "32469  {'word': '화순군', 'start_idx': 0, 'end_idx': 2, ...   \n",
       "\n",
       "                                           object_entity  \\\n",
       "0      {'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...   \n",
       "1      {'word': '대안신당', 'start_idx': 14, 'end_idx': 1...   \n",
       "2      {'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...   \n",
       "3      {'word': '박정부', 'start_idx': 22, 'end_idx': 24...   \n",
       "4      {'word': '1967', 'start_idx': 0, 'end_idx': 3,...   \n",
       "...                                                  ...   \n",
       "32465  {'word': '부산 서구·동구', 'start_idx': 100, 'end_id...   \n",
       "32466  {'word': '손병희', 'start_idx': 17, 'end_idx': 19...   \n",
       "32467  {'word': '신우철', 'start_idx': 7, 'end_idx': 9, ...   \n",
       "32468  {'word': '중앙홀딩스', 'start_idx': 21, 'end_idx': ...   \n",
       "32469  {'word': '구충곤', 'start_idx': 7, 'end_idx': 9, ...   \n",
       "\n",
       "                           label     source  \n",
       "0                    no_relation  wikipedia  \n",
       "1                    no_relation   wikitree  \n",
       "2                  org:member_of   wikitree  \n",
       "3      org:top_members/employees   wikitree  \n",
       "4                    no_relation  wikipedia  \n",
       "...                          ...        ...  \n",
       "32465            per:employee_of   wikitree  \n",
       "32466             per:colleagues  wikipedia  \n",
       "32467  org:top_members/employees   wikitree  \n",
       "32468                no_relation  wikipedia  \n",
       "32469  org:top_members/employees   wikitree  \n",
       "\n",
       "[32470 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_df = pd.read_csv(\"/opt/ml/dataset/train/train.csv\")\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841ee3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>비틀즈</td>\n",
       "      <td>조지 해리슨</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>민주평화당</td>\n",
       "      <td>대안신당</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>광주FC</td>\n",
       "      <td>한국프로축구연맹</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>아성다이소</td>\n",
       "      <td>박정부</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>요미우리 자이언츠</td>\n",
       "      <td>1967</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32465</th>\n",
       "      <td>32465</td>\n",
       "      <td>한국당은 7일 오전 9시부터 오후 5시까지 진행된 원내대표 및 정책위의장 후보자 등...</td>\n",
       "      <td>유기준</td>\n",
       "      <td>부산 서구·동구</td>\n",
       "      <td>per:employee_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32466</th>\n",
       "      <td>32466</td>\n",
       "      <td>법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...</td>\n",
       "      <td>최시형</td>\n",
       "      <td>손병희</td>\n",
       "      <td>per:colleagues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32467</th>\n",
       "      <td>32467</td>\n",
       "      <td>완도군(군수 신우철)이 국토교통부에서 실시한 '2019 교통문화지수 실태조사'에서 ...</td>\n",
       "      <td>완도군</td>\n",
       "      <td>신우철</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32468</th>\n",
       "      <td>32468</td>\n",
       "      <td>중앙일보, JTBC 회장을 지낸 이후 중앙홀딩스 회장, 재단법인 한반도평화만들기 이...</td>\n",
       "      <td>JTBC</td>\n",
       "      <td>중앙홀딩스</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32469</th>\n",
       "      <td>32469</td>\n",
       "      <td>화순군(군수 구충곤)은 17일 동면의 이장 20여 명이 코로나 19 예방을 위해 버...</td>\n",
       "      <td>화순군</td>\n",
       "      <td>구충곤</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "0          0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...   \n",
       "1          1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...   \n",
       "2          2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...   \n",
       "3          3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...   \n",
       "4          4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...   \n",
       "...      ...                                                ...   \n",
       "32465  32465  한국당은 7일 오전 9시부터 오후 5시까지 진행된 원내대표 및 정책위의장 후보자 등...   \n",
       "32466  32466  법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...   \n",
       "32467  32467  완도군(군수 신우철)이 국토교통부에서 실시한 '2019 교통문화지수 실태조사'에서 ...   \n",
       "32468  32468  중앙일보, JTBC 회장을 지낸 이후 중앙홀딩스 회장, 재단법인 한반도평화만들기 이...   \n",
       "32469  32469  화순군(군수 구충곤)은 17일 동면의 이장 20여 명이 코로나 19 예방을 위해 버...   \n",
       "\n",
       "      subject_entity object_entity                      label  \n",
       "0                비틀즈        조지 해리슨                no_relation  \n",
       "1              민주평화당          대안신당                no_relation  \n",
       "2               광주FC      한국프로축구연맹              org:member_of  \n",
       "3              아성다이소           박정부  org:top_members/employees  \n",
       "4          요미우리 자이언츠          1967                no_relation  \n",
       "...              ...           ...                        ...  \n",
       "32465            유기준      부산 서구·동구            per:employee_of  \n",
       "32466            최시형           손병희             per:colleagues  \n",
       "32467            완도군           신우철  org:top_members/employees  \n",
       "32468           JTBC         중앙홀딩스                no_relation  \n",
       "32469            화순군           구충곤  org:top_members/employees  \n",
       "\n",
       "[32470 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_entity = []\n",
    "object_entity = []\n",
    "for i,j in zip(total_df['subject_entity'], total_df['object_entity']):\n",
    "    i = eval(i)['word']\n",
    "    j = eval(j)['word']\n",
    "\n",
    "    subject_entity.append(i)\n",
    "    object_entity.append(j)\n",
    "out_dataset = pd.DataFrame({'id':total_df['id'], 'sentence':total_df['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':total_df['label'],})\n",
    "out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737bcce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '호시조라', '미유키', '\"(', 'みゆき', '한', '국명', ':', '김다솜', ')', '는', '토에이', '애니메이션', '제작', '의', '애니메이션', '《', '스마일', '프리큐어', '!', '》', '에', '등장', '하', '는', '가공', '의', '인물', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "morphs = mecab.morphs(out_dataset['sentence'][2725] + \"\")\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f18e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'ᄒ',\n",
       " '##ᅩ',\n",
       " '##ᄉ',\n",
       " '##ᅵ',\n",
       " '##ᄌ',\n",
       " '##ᅩ',\n",
       " '##ᄅ',\n",
       " '##ᅡ',\n",
       " 'ᄆ',\n",
       " '##ᅵ',\n",
       " '##ᄋ',\n",
       " '##ᅲ',\n",
       " '##ᄏ',\n",
       " '##ᅵ',\n",
       " '\"',\n",
       " '(',\n",
       " 'み',\n",
       " '##ゆ',\n",
       " '##き',\n",
       " 'ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᆫ',\n",
       " '##ᄀ',\n",
       " '##ᅮ',\n",
       " '##ᆨ',\n",
       " '##ᄆ',\n",
       " '##ᅧ',\n",
       " '##ᆼ',\n",
       " ':',\n",
       " 'ᄀ',\n",
       " '##ᅵ',\n",
       " '##ᆷ',\n",
       " '##ᄃ',\n",
       " '##ᅡ',\n",
       " '##ᄉ',\n",
       " '##ᅩ',\n",
       " '##ᆷ',\n",
       " ')',\n",
       " 'ᄂ',\n",
       " '##ᅳ',\n",
       " '##ᆫ',\n",
       " 'ᄐ',\n",
       " '##ᅩ',\n",
       " '##ᄋ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " 'ᄋ',\n",
       " '##ᅢ',\n",
       " '##ᄂ',\n",
       " '##ᅵ',\n",
       " '##ᄆ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄉ',\n",
       " '##ᅧ',\n",
       " '##ᆫ',\n",
       " 'ᄌ',\n",
       " '##ᅦ',\n",
       " '##ᄌ',\n",
       " '##ᅡ',\n",
       " '##ᆨ',\n",
       " '##ᄋ',\n",
       " '##ᅴ',\n",
       " 'ᄋ',\n",
       " '##ᅢ',\n",
       " '##ᄂ',\n",
       " '##ᅵ',\n",
       " '##ᄆ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄉ',\n",
       " '##ᅧ',\n",
       " '##ᆫ',\n",
       " '《',\n",
       " 'ᄉ',\n",
       " '##ᅳ',\n",
       " '##ᄆ',\n",
       " '##ᅡ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᆯ',\n",
       " 'ᄑ',\n",
       " '##ᅳ',\n",
       " '##ᄅ',\n",
       " '##ᅵ',\n",
       " '##ᄏ',\n",
       " '##ᅲ',\n",
       " '##ᄋ',\n",
       " '##ᅥ',\n",
       " '!',\n",
       " '》',\n",
       " 'ᄋ',\n",
       " '##ᅦ',\n",
       " 'ᄃ',\n",
       " '##ᅳ',\n",
       " '##ᆼ',\n",
       " '##ᄌ',\n",
       " '##ᅡ',\n",
       " '##ᆼ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᄂ',\n",
       " '##ᅳ',\n",
       " '##ᆫ',\n",
       " 'ᄀ',\n",
       " '##ᅡ',\n",
       " '##ᄀ',\n",
       " '##ᅩ',\n",
       " '##ᆼ',\n",
       " '##ᄋ',\n",
       " '##ᅴ',\n",
       " 'ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᆫ',\n",
       " '##ᄆ',\n",
       " '##ᅮ',\n",
       " '##ᆯ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄃ',\n",
       " '##ᅡ',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# TOK_NAME = \"klue/bert-base\"\n",
    "TOK_NAME = \"bert-base-uncased\"\n",
    "bert_base_tokenizer = BertTokenizer(\"wordPieceTokenizer/vocab_bert_base.txt\")\n",
    "bert_base_tokenizer.tokenize(out_dataset['sentence'][2725])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a49b213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'ᄒ',\n",
       " '##ᅩ',\n",
       " '##ᄉ',\n",
       " '##ᅵ',\n",
       " '##ᄌ',\n",
       " '##ᅩ',\n",
       " '##ᄅ',\n",
       " '##ᅡ',\n",
       " 'ᄆ',\n",
       " '##ᅵ',\n",
       " '##ᄋ',\n",
       " '##ᅲ',\n",
       " '##ᄏ',\n",
       " '##ᅵ',\n",
       " '\"',\n",
       " '(',\n",
       " 'み',\n",
       " '##ゆ',\n",
       " '##き',\n",
       " 'ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᆫ',\n",
       " '##ᄀ',\n",
       " '##ᅮ',\n",
       " '##ᆨ',\n",
       " '##ᄆ',\n",
       " '##ᅧ',\n",
       " '##ᆼ',\n",
       " ':',\n",
       " 'ᄀ',\n",
       " '##ᅵ',\n",
       " '##ᆷ',\n",
       " '##ᄃ',\n",
       " '##ᅡ',\n",
       " '##ᄉ',\n",
       " '##ᅩ',\n",
       " '##ᆷ',\n",
       " ')',\n",
       " 'ᄂ',\n",
       " '##ᅳ',\n",
       " '##ᆫ',\n",
       " 'ᄐ',\n",
       " '##ᅩ',\n",
       " '##ᄋ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " 'ᄋ',\n",
       " '##ᅢ',\n",
       " '##ᄂ',\n",
       " '##ᅵ',\n",
       " '##ᄆ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄉ',\n",
       " '##ᅧ',\n",
       " '##ᆫ',\n",
       " 'ᄌ',\n",
       " '##ᅦ',\n",
       " '##ᄌ',\n",
       " '##ᅡ',\n",
       " '##ᆨ',\n",
       " '##ᄋ',\n",
       " '##ᅴ',\n",
       " 'ᄋ',\n",
       " '##ᅢ',\n",
       " '##ᄂ',\n",
       " '##ᅵ',\n",
       " '##ᄆ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄉ',\n",
       " '##ᅧ',\n",
       " '##ᆫ',\n",
       " '《',\n",
       " 'ᄉ',\n",
       " '##ᅳ',\n",
       " '##ᄆ',\n",
       " '##ᅡ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᆯ',\n",
       " 'ᄑ',\n",
       " '##ᅳ',\n",
       " '##ᄅ',\n",
       " '##ᅵ',\n",
       " '##ᄏ',\n",
       " '##ᅲ',\n",
       " '##ᄋ',\n",
       " '##ᅥ',\n",
       " '!',\n",
       " '》',\n",
       " 'ᄋ',\n",
       " '##ᅦ',\n",
       " 'ᄃ',\n",
       " '##ᅳ',\n",
       " '##ᆼ',\n",
       " '##ᄌ',\n",
       " '##ᅡ',\n",
       " '##ᆼ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᄂ',\n",
       " '##ᅳ',\n",
       " '##ᆫ',\n",
       " 'ᄀ',\n",
       " '##ᅡ',\n",
       " '##ᄀ',\n",
       " '##ᅩ',\n",
       " '##ᆼ',\n",
       " '##ᄋ',\n",
       " '##ᅴ',\n",
       " 'ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᆫ',\n",
       " '##ᄆ',\n",
       " '##ᅮ',\n",
       " '##ᆯ',\n",
       " '##ᄋ',\n",
       " '##ᅵ',\n",
       " '##ᄃ',\n",
       " '##ᅡ',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_base_pre_tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "bert_base_tokenizer.tokenize(out_dataset['sentence'][2725])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "facb5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from tokenizer.sentencepiece import SentencePieceTokenizer\n",
    "from tokenizer.mecab import MeCabTokenizer\n",
    "from tokenizer.mecab_sp import MeCabSentencePieceTokenizer\n",
    "from tokenization import BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "tokenizer_dir = os.path.join(\"../resources\", \"mecab_sp-64k\")\n",
    "mecab = MeCabTokenizer(os.path.join(tokenizer_dir, \"tok.json\")) \n",
    "sp = SentencePieceTokenizer(os.path.join(tokenizer_dir, \"tok.model\"))\n",
    "custom_tokenizer = MeCabSentencePieceTokenizer(mecab, sp)\n",
    "tokenizer = BertTokenizer(os.path.join(tokenizer_dir, \"tok.vocab.txt\"), custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a4b8edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁\"',\n",
       " '▁호시',\n",
       " '조',\n",
       " '라',\n",
       " '▁미유키',\n",
       " '▁\"(',\n",
       " '▁',\n",
       " 'みゆき',\n",
       " '▁한',\n",
       " '▁국명',\n",
       " '▁:',\n",
       " '▁김',\n",
       " '다',\n",
       " '솜',\n",
       " '▁)',\n",
       " '▁는',\n",
       " '▁토에이',\n",
       " '▁애니메이션',\n",
       " '▁제작',\n",
       " '▁의',\n",
       " '▁애니메이션',\n",
       " '▁',\n",
       " '《',\n",
       " '▁스마일',\n",
       " '▁프리큐어',\n",
       " '▁!',\n",
       " '▁',\n",
       " '》',\n",
       " '▁에',\n",
       " '▁등장',\n",
       " '▁하',\n",
       " '▁는',\n",
       " '▁가공',\n",
       " '▁의',\n",
       " '▁인물',\n",
       " '▁이',\n",
       " '▁다',\n",
       " '▁.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer.tokenize(out_dataset['sentence'][2725])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d21cf28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '▁\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10592/2945018985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2725\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3177\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3179\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3180\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3181\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '▁\"'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.tokenize(out_dataset['sentence'][2725]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "772d7116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    4,  6727,   531,  ...,     0,     0,     0],\n",
       "        [    4,   607,  1699,  ...,     0,     0,     0],\n",
       "        [    4,  1623,  1045,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    4,   229,  1981,  ...,     0,     0,     0],\n",
       "        [    4,   797, 61031,  ...,     0,     0,     0],\n",
       "        [    4,  1019,   490,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.tokenize(out_dataset['sentence'][0:60])\n",
    "\n",
    "concat_entity = []\n",
    "for e01, e02 in zip(out_dataset['subject_entity'][0:60], out_dataset['object_entity'][0:60]):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "\n",
    "\n",
    "tokenizer(\n",
    "      concat_entity,\n",
    "      list(out_dataset['sentence'][0:60]),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f9fc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS][UNK]Something[UNK]는조지해리슨이쓰고비틀즈가1969년앨범[UNK]AbbeyRoad[UNK]에담은노래다.[SEP]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(out_dataset['sentence'][0]))\n",
    "# tokenizer.tokenize(out_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac28851d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '〈',\n",
       " '▁Something',\n",
       " '▁',\n",
       " '〉',\n",
       " '▁는',\n",
       " '▁조지',\n",
       " '▁해리슨',\n",
       " '▁이',\n",
       " '▁쓰',\n",
       " '▁고',\n",
       " '▁비틀즈',\n",
       " '▁가',\n",
       " '▁1969',\n",
       " '▁년',\n",
       " '▁앨범',\n",
       " '▁',\n",
       " '《',\n",
       " '▁Abbey',\n",
       " '▁Road',\n",
       " '▁',\n",
       " '》',\n",
       " '▁에',\n",
       " '▁담',\n",
       " '▁은',\n",
       " '▁노래',\n",
       " '▁다',\n",
       " '▁.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(out_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e0a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_bert_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdfe616d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['〈',\n",
       " 'So',\n",
       " '##me',\n",
       " '##th',\n",
       " '##ing',\n",
       " '〉',\n",
       " '는',\n",
       " '조지',\n",
       " '해리',\n",
       " '##슨',\n",
       " '##이',\n",
       " '쓰',\n",
       " '##고',\n",
       " '비틀즈',\n",
       " '##가',\n",
       " '1969',\n",
       " '##년',\n",
       " '앨범',\n",
       " '《',\n",
       " 'Ab',\n",
       " '##be',\n",
       " '##y',\n",
       " 'Ro',\n",
       " '##ad',\n",
       " '》',\n",
       " '에',\n",
       " '담',\n",
       " '##은',\n",
       " '노래',\n",
       " '##다',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# klue_bert_tokenizer.decode(klue_bert_tokenizer.encode(out_dataset['sentence'][0]))\n",
    "klue_bert_tokenizer.tokenize(out_dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2b5a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 60921, 1, 48077, 60921, 1, 13, 1410, 11499, 8, 385, 22, 6727, 19, 2597, 25, 765, 60921, 1, 50213, 15116, 60921, 1, 10, 559, 18, 1006, 11, 9, 5]\n",
      "[2, 168, 30985, 14451, 7088, 4586, 169, 793, 8373, 14113, 2234, 2052, 1363, 2088, 29830, 2116, 14879, 2440, 6711, 170, 21406, 26713, 2076, 25145, 5749, 171, 1421, 818, 2073, 4388, 2062, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(out_dataset['sentence'][0]))\n",
    "print(klue_bert_tokenizer.encode(out_dataset['sentence'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8a2e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4,\n",
       "  74,\n",
       "  17523,\n",
       "  61002,\n",
       "  60961,\n",
       "  29421,\n",
       "  1606,\n",
       "  60921,\n",
       "  1,\n",
       "  24,\n",
       "  17701,\n",
       "  274,\n",
       "  222,\n",
       "  60924,\n",
       "  62024,\n",
       "  21,\n",
       "  13,\n",
       "  31735,\n",
       "  1341,\n",
       "  492,\n",
       "  12,\n",
       "  1341,\n",
       "  60921,\n",
       "  1,\n",
       "  31528,\n",
       "  17547,\n",
       "  968,\n",
       "  60921,\n",
       "  1,\n",
       "  10,\n",
       "  724,\n",
       "  15,\n",
       "  13,\n",
       "  4263,\n",
       "  12,\n",
       "  1044,\n",
       "  8,\n",
       "  11,\n",
       "  9,\n",
       "  5],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "from functools import partial\n",
    "from typing import List, Optional, Tuple, TypeVar\n",
    "\n",
    "from transformers import BertConfig, BertForPreTraining, BertModel\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab_path: str, pad_token: str = \"[PAD]\", unk_token: str = \"[UNK]\"):\n",
    "        \"\"\"\n",
    "        Vocab Constructor\n",
    "        :param vocab_path: 로드할 vocab path\n",
    "        :param pad_token: 사용할 PAD 토큰\n",
    "        :param unk_token: 사용할 UNK 토큰\n",
    "        \"\"\"\n",
    "        self.__vocab: Dict[str, int] = self._load_vocab_file(vocab_path)\n",
    "        self.__inv_vocab: Dict[int, str] = {v: k for k, v in self.__vocab.items()}\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_token_id = self.__vocab[self.pad_token]\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_token_id = self.__vocab[self.unk_token]\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.__vocab\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__vocab.keys())\n",
    "\n",
    "    def get_vocab(self) -> List[str]:\n",
    "        return list(self.__vocab.keys())\n",
    "\n",
    "    def convert_token_to_id(self, token: str, default: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        토큰 하나를 인덱스로 바꾸는 함수\n",
    "        :param token: 바꿀 토큰\n",
    "        :param default: 만약 해당 토큰이 vocab에 없다면 반환할 default 값\n",
    "        :raise KeyError: ``default`` 없이 unk 토큰이 넘어왔을 때\n",
    "        \"\"\"\n",
    "        if default:\n",
    "            return self.__vocab.get(token, default)\n",
    "        return self.__vocab[token]\n",
    "\n",
    "    def convert_id_to_token(self, idx: int, default: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        인덱스 하나를 토큰으로 바꾸는 함수\n",
    "        :param idx: 바꿀 인덱스\n",
    "        :param default: 만약 해당 인덱스가 vocab에 없다면 반환할 default 값\n",
    "        :raise KeyError: ``default`` 없이 알 수 없는 인덱스가 넘어왔을 때\n",
    "        \"\"\"\n",
    "        if default:\n",
    "            return self.__inv_vocab.get(idx, default)\n",
    "        return self.__inv_vocab[idx]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: Iterable[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        토큰 여러개를 인덱스들로 바꾸는 함수\n",
    "        :param tokens: 바꿀 토큰들\n",
    "        \"\"\"\n",
    "        return [self.__vocab.get(item, self.unk_token_id) for item in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids: Iterable[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        인덱스 여러개를 토큰들로 바꾸는 함수\n",
    "        :param idx: 바꿀 인덱스들\n",
    "        \"\"\"\n",
    "        return [self.__inv_vocab[item] for item in ids]\n",
    "\n",
    "    def dump(self, vocab_path: str):\n",
    "        with open(vocab_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(self.__vocab.keys()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_vocab_file(vocab_path: str) -> Dict[str, int]:\n",
    "        vocab: Dict[str, int] = OrderedDict()\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            for index, token in enumerate(f):\n",
    "                token = token.strip().split(\"\\t\")[0]\n",
    "\n",
    "                if token in vocab:\n",
    "                    raise ValueError(f\"Vocab에 중복된 토큰 {token}이 있습니다.\")\n",
    "\n",
    "                vocab[token] = index\n",
    "\n",
    "        return vocab\n",
    "    \n",
    "    \n",
    "    \n",
    "def convert_single_to_feature(\n",
    "    sentence: str, tokenizer: BertTokenizer, vocab: Vocab, max_length: int\n",
    ") -> Tuple[List[int], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    sentence 하나를 feature로 바꾸어주는 함수입니다.\n",
    "    :param sentence: 변환할 sentence.\n",
    "    :param pipeline: pnlp의 NLPPipeline으로, 전처리 및 tokenize pipeline. (tokenize는 필수로 포함되어야 함.)\n",
    "    :param vocab: token을 indexing하기 위한 Vocab ( ``pnlp.Vocab`` )\n",
    "    :param max_sequence_length: 넘지 말아야 할 최대 길이\n",
    "    :return: 변환된 feature. (``Tuple[List[int], List[int], List[int], List[int]]``)\n",
    "    \"\"\"\n",
    "    example = tokenizer.tokenize(sentence)\n",
    "    example = example[: max_length - 2]\n",
    "    example = [\"[CLS]\"] + example + [\"[SEP]\"]\n",
    "\n",
    "    token_ids = vocab.convert_tokens_to_ids(example)\n",
    "    attention_mask = [1] * len(token_ids)\n",
    "    token_type_ids = [0] * len(token_ids)\n",
    "\n",
    "    return (token_ids, attention_mask, token_type_ids)\n",
    "\n",
    "\n",
    "tokenizer_dir = os.path.join(\"../resources\", \"mecab_sp-64k\")\n",
    "vocab = Vocab(os.path.join(tokenizer_dir, \"tok.vocab.txt\"))\n",
    "input_features = convert_single_to_feature(out_dataset['sentence'][2725], tokenizer, vocab, 256)\n",
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d2e8cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]\"호시조라미유키\"([UNK]한국명:김다솜)는토에이애니메이션제작의애니메이션[UNK]스마일프리큐어![UNK]에등장하는가공의인물이다.[SEP]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c212e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁\"',\n",
       " '▁호시',\n",
       " '조',\n",
       " '라',\n",
       " '▁미유키',\n",
       " '▁\"(',\n",
       " '▁',\n",
       " 'みゆき',\n",
       " '▁한',\n",
       " '▁국명',\n",
       " '▁:',\n",
       " '▁김',\n",
       " '다',\n",
       " '솜',\n",
       " '▁)',\n",
       " '▁는',\n",
       " '▁토에이',\n",
       " '▁애니메이션',\n",
       " '▁제작',\n",
       " '▁의',\n",
       " '▁애니메이션',\n",
       " '▁',\n",
       " '《',\n",
       " '▁스마일',\n",
       " '▁프리큐어',\n",
       " '▁!',\n",
       " '▁',\n",
       " '》',\n",
       " '▁에',\n",
       " '▁등장',\n",
       " '▁하',\n",
       " '▁는',\n",
       " '▁가공',\n",
       " '▁의',\n",
       " '▁인물',\n",
       " '▁이',\n",
       " '▁다',\n",
       " '▁.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer.tokenize(out_dataset['sentence'][2725])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
